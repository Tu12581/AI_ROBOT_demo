" 简历 论文与专利     施少怀，哈尔滨工业大学（深圳）计算机科学与技术学院教授、博士生导师，2022年入选国家级青年人才计划，“鹏城孔雀计划”特聘岗位B档。2020年在香港浸会大学获得博士学位，2020-2022年在香港科技大学计算机科学与工程系任研究助理教授。研究兴趣为分布式机器学习系统和高性能计算，在相关领域共发表文章30余篇，包括TPDS，INFOCOM，ICLR，MLSys等顶刊或顶会论文，并授权1 项美国专利。2篇论文分别获得国际会议IEEE DataCom 2018和IEEE INFOCOM 2021最佳论文奖；总谷歌学术引用超过2400次，H-index为23；其中一个代表性著作成功应用于超大规模2048块GPU的集群，于2018年，在百万级图片识别训练任务的训练时间上创造了最快的世界纪录。他也担任多个学术服务，包含担任ACM MobiSys 2021研讨会 EMDL程序委员会共同主席、多个顶会(如NeurIPS, ICLR, ICDCS等)程序委员会成员以及多个顶刊（如TPDS, TMC, TNSE等）审稿人。 More updated information can be found on my English homepage: https://shaohuais.github.io/ 招收2025级博士研究生2名（2025年春季或秋季入学），也非常欢迎直博或硕博联读学生，有兴趣者可以邮件联系: shaohuais@hit.edu.cn。     10/2023至今: 哈尔滨工业大学（深圳），计算机科学与技术学院，教授 09/2022-09/2023: 哈尔滨工业大学（深圳），计算机科学与技术学院，助理教授 09/2020-08/2022: 香港科技大学，计算机科学与工程系，研究助理教授 04/2019-05/2020: NVIDIA香港研究中心，研究实习生 02/2014-03/2016: 香港浸会大学，理学院院长办公室，高级研究助理 02/2013-02/2014: 香港浸会大学，理学院院长办公室，研究助理     03/2016-08/2020: 香港浸会大学，计算机科学，博士 09/2010-01/2013: 哈尔滨工业大学，计算机科学与技术，硕士（保研） 09/2006-07/2010: 华南理工大学，软件工程，学士 教学    2023秋季: 并行处理与体系结构，研究生 2023秋季: 计算机体系结构，本科生 2022春季: 计算机系统，本科生 2022 Spring Semester: High Performace Computing, HKUST 2021 Spring Semester: High Performace Computing, HKUST 已    [41] Y. Luo, Q. Wang, S. Shi, J. Lai, S. Qi, J. Zhang, and X. Wang, “Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise Resource Sharing,” IEEE/ACM International Symposium on Quality of Service (IWQoS 2024), Guangzhou, China, June 19-21, 2024. [40] Z. Tang, Y. Zhang, S. Shi, X. Tian, T. Liu, B. Han, X.-W. Chu, “FedImpro: Measuring and Improving Client Update in Federated Learning,” International Conference on Learning Representations (ICLR) 2024, Vienna, Austria, May 7-11, 2024. [39] S. Shi, X. Pan, Q. Wang, C. Liu, X. Ren, Z. Hu, Y. Yang, B. Li, and X.-W. Chu, “ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling,” European Conference on Computer Systems (EuroSys) 2024, Athens, Greece, April 22-25, 2024. [38] X. Pan, W. Lin, S. Shi, X.-W. Chu, W. Sun, and B. Li, “Parm: Efficient Training of Large Sparsely-Activated Models with Dedicated Schedules,” IEEE International Conference on Computer Communications (INFOCOM) 2024, Vancouver, Canada, May 20-23, 2024. [37] H. Liu, S. Shi, X. Wang, Z.L. Jiang, and Q. Chen, “Performance Analysis and Optimizations of Matrix Multiplications on ARMv8 Processors,” Design, Automation and Test in Europe Conference (DATE), Valencia, Spain, March 25-27, 2024. [36] Z. Tang, Y. Wang, X. He, L. Zhang, X. Pan, Q. Wang, R. Zeng, S. Shi, B. He, and X.-W. Chu, “FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,” Symposium on Large Language Models (LLM 2023) with IJCAI 2023, Macao, China, August 21, 2023. [35] L. Zhang, L. Zhang, S. Shi, X.-W. Chu, and B. Li, “Evaluation and Optimization of Gradient Compression for Distributed Deep Learning,” The 43rd IEEE International Conference on Distributed Computing Systems (ICDCS), Hong Kong, China, July 2023. [34] L. Zhang, S. Shi, X.-W Chu, W. Wang, B. Li, and C. Liu, “Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining,” The 43rd IEEE International Conference on Distributed Computing Systems (ICDCS), Hong Kong, China, July 2023. [33] S. Shi, Q. Yang, Y. Xiang, S. Qi, and X. Wang, “An Efficient Split Fine-tuning Framework for Edge and Cloud Collaborative Learning,” The Design Automation Conference (DAC) 2023 (Poster), Moscone West, San Francisco, July 2023. [32] L. Zhang, S. Shi, and B. Li, “Eva: Practical Second-order Optimization with Kronecker-vectorized Approximation,” International Conference on Learning Representations (ICLR) 2023, Kigali, Rwanda, May 2023. [31] L. Zhang, S. Shi, and B. Li, “Accelerating Distributed K-FAC with Efficient Collective Communication and Scheduling,” IEEE International Conference on Computer Communications (INFOCOM) 2023, New York Area, U.S.A., May 2023. [30] S. Shi, X. Pan, X.-W. Chu, and B. Li, “PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining,” IEEE International Conference on Computer Communications (INFOCOM) 2023, New York Area, U.S.A., May 2023. [29] Z. Tang, S. Shi, B. Li, and X.-W. Chu, “GossipFL: A Decentralized Federated Learning Framework with Sparsified and Adaptive Communication,” IEEE Transactions on Parallel and Distributed Systems (TPDS), vol. 34, no. 3, pp. 909-922, March 2023. [28] L. Zhang, S. Shi, W. Wang, and B. Li, “Scalable K-FAC Training for Deep Neural Networks with Distributed Preconditioning,” IEEE Transactions on Cloud Computing (TCC), September 2022. [27] Q. Wang, S. Shi, K. Zhao, X.-W. Chu, “EASNet: Searching Elastic and Accurate Network Architecture for Stereo Matching,” European Conference on Computer Vision (ECCV), Tel Aviv, Israel, October 2022. [26] Tang, Y. Zhang, S. Shi, X. He, B. Han, X.-W. Chu, “Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,” The 39th International Conference on Machine Learning (ICML), Baltimore MD, July 2022. [25] Z. Tang, Z. Hu, S. Shi, Y.-M. Cheung, Y. Jin, Z. Ren, and X.-W. Chu, “Data Resampling for Federated Learning with Non-IID Labels,” International Workshop on Federated and Transfer Learning for Data Sparsity and Confidentiality in Conjunction with IJCAI 2021 (FTL-IJCAI'21), Virtual Event, August 2021. [24] S. Shi, L. Zhang, and B. Li, “Accelerating Distributed K-FAC with Smart Parallelism of Computing and Communication Tasks,” The 41st IEEE International Conference on Distributed Computing Systems (ICDCS), Virtual Event, July 2021. [23] S. Shi, X.-W. Chu, and B. Li, “Exploiting Simultaneous Communications to Accelerate Data Parallel Distributed Deep Learning,” IEEE International Conference on Computer Communications (INFOCOM) 2021, Virtual Event, May 2021. (Best  Award, 3/1266) [22] S. Shi*, X. Zhou*, S. Song*, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou, Z. Guo, L. Xie, R. Lan, X. Ouyang, Y. Zhang, J. Wei, J. Gong, W. Lin, P. Gao, P. Meng, X. Xu, C. Guo, B. Yang, Z. Chen, Y. Wu, and X.-W. Chu, “Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters,” The 4th Conference on Machine Learning and Systems (MLSys), Virtual Event, April 2021. (* means equal contribution) [21] X. He, S. Wang, X.-W. Chu, S. Shi, J. Tang, X. Liu, C. Yan, J. Zhang, and G. Ding, “Automated Model Design and Benchmarking of Deep Learning Models for COVID-19 Detection with Chest CT Scans,” The 35th AAAI Conference on Artificial Intelligence (AAAI), Virtual Event, February 2021. [20] S. Shi, X.-W. Chu, and B. Li, “MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning,” IEEE Transactions on Parallel and Distributed Systems (TPDS), vol. 32, no. 8, pp. 1903-1917, January 2021. [19] S. Shi, Z. Tang, X.-W. Chu, C. Liu, W. Wang, and B. Li, “A Quantitative Survey of Communication Optimizations in Distributed Deep Learning,” IEEE Network, vol. 35, no. 3, pp. 230-237, December 2020. [18] Z. Tang, S. Shi, and X.-W. Chu, “Communication-Efficient Decentralized Learning with Sparsification and Adaptive Peer Selection,” The 40th IEEE International Conference on Distributed Computing Systems (ICDCS) (Poster), Singapore, December 2020. [17] S. Shi, Q. Wang, and X.-W. Chu, “Efficient Sparse-Dense Matrix-Matrix Multiplication on GPUs Using the Customized Sparse Storage Format,” The 26th International Conference on Parallel and Distributed Systems (ICPADS) 2020, Hong Kong, December 2020. [16] S. Shi, Q. Wang, X.-W. Chu, B. Li, Y. Qin, R. Liu, and X. Zhao, “Communication-Efficient Distributed Deep Learning with Merged Gradient Sparsification on GPUs,” IEEE International Conference on Computer Communications (INFOCOM) 2020, Toronto, Canada, July 2020. [15] Q. Wang*, S. Shi*, S. Zheng, K. Zhao, and X.-W. Chu, “FADNet: A Fast and Accurate Network for Disparity Estimation,” International Conference on Robotics and Automation (ICRA), Paris, France, June 2020. (* means equal contribution) [14] S. Shi, Z. Tang, Q. Wang, K. Zhao, and X.-W. Chu, “Layer-wise Adaptive Gradient Sparsification for Distributed Deep Learning with Convergence Guarantees,” The 24th European Conference on Artificial Intelligence (ECAI), Santiago de Compostela, Spain, June 2020. [13] Y. Wang, Q. Wang, S. Shi, X. He, Z. Tang, K. Zhao, and X.-W. Chu, “Benchmarking the Performance and Energy Efficiency of AI Accelerators for AI Training,” The 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID), Melbourne, Australia, May 2020. [12] X. He, S. Wang, S. Shi, Z. Tang, Y. Wang, Z. Zhao, J. Dai, R. Ni, X. Zhang, X. Liu, Z. Wu, W. Yu, and X.-W. Chu, “Computer-Aided Clinical Skin Disease Diagnosis Using CNN and Object Detection Models,” KDDBHI Workshop 2019, IEEE BigData Conference, Los Angeles, USA, December 2019. [11] S. Shi, K. Zhao, Q. Wang, Z. Tang, and X.-W. Chu, “A Convergence Analysis of Distributed SGD with Communication-Efficient Gradient Sparsification,” The 28th International Joint Conference on Artificial Intelligence (IJCAI), Macao, China, August 2019. [10] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X.-W. Chu, “A Distributed Synchronous SGD Algorithm with Global Top-k Sparsification for Low Bandwidth Networks,” The 39th IEEE International Conference on Distributed Computing Systems (ICDCS), Texas, USA, July 2019. [9] S. Shi, X.-W. Chu, and B. Li, “MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD Algorithms,” IEEE International Conference on Computer Communications (INFOCOM) 2019, Paris, France, May 2019. [8] X. Jia*, S. Song*, S. Shi*, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z. Guo, Y. Yang, L. Yu, T. Chen, G. Hu, and X.-W. Chu, “Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes,” NeurIPS 2018 Workshop on Systems for ML and Open Source Software, Montreal, Canada, December 2018. (* means equal contribution) (Google Scholar Citations: 400+) [7] S. Shi, Q. Wang, X.-W. Chu, and B. Li, “A DAG Model of Synchronous Stochastic Gradient Descent in Distributed Deep Learning,” The 24th International Conference on Parallel and Distributed Systems (ICPADS), Sentosa, Singapore, December 2018. [6] S. Shi, Q. Wang, and X.-W. Chu, “Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs,” The 4th IEEE International Conference on Big Data Intelligence and Computing (DataCom) 2018, Athens, Greece, August 2018. (Best  Award) [5] S. Shi, P. Xu, and X.-W. Chu, “Supervised Learning Based Algorithm Selection for Deep Neural Networks,” The 23rd International Conference on Parallel and Distributed Systems (ICPADS), Shenzhen, China, December 2017. [4] P. Xu, S. Shi, and X.-W. Chu, “Performance Evaluation of Deep Learning Tools in Docker Containers,” The 3rd International Conference on Big Data Computing and Communications (BigCom), Chengdu, China, August 2017. [3] S. Shi, Q. Wang, P. Xu, and X.-W. Chu, “Benchmarking State-of-the-art Deep Learning Software Tools,” The 7th International Conference on Cloud Computing and Big Data (CCBD), Macao, China, November 2016. (Google Scholar Citations: 430+) [2] S. Qi, X. Wang, and S. Shi, “Mixed Precision Method for GPU-based FFT,” The 14th IEEE International Conference on Computational Science and Engineering, Dalian, China, August 2011. [1] J. Peng, H. Chen, and S. Shi, “The GPU-based String Matching System in Advanced AC Algorithm,” The 10th IEEE International Conference on Computer and Information Technology, Bradford, UK, June 2010.     X.-W. Chu, S. Shi, and K. Zhao, “System for Efficient Large-scale Data Distribution in Distributed and Parallel Processing Environment,” US Patent, US 11436065, September 2022. "
